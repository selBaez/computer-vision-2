\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{float}


\title{
	{Computer Vision 2 - Assignment 1\\
	 Iterative Closest Point - ICP}
}
\author{
Selene Baez Santamaria (10985417) - Ildefonso Ferreira (10254757)}
\date{\today}

\begin{document}

\maketitle

\section{ICP}
In this assignment we create a function called \textit{own\_ICP} which implements the Iterative Closest Point algorithm. This function takes as arguments a base point cloud and a target point cloud, and a parameter $k$ for the number of points to be sampled. It outputs the base point cloud, the transformed point cloud, and the rotation and transformation matrices.

\subsection{Implementation}
Our implementation follows the basic steps outlined by \texttt{Cai et al}. The steps are further explained in the following list:

\begin{enumerate}
	\item \textbf{Find closest points:} In this phase we find the corresponding points in the clouds. To do so we use the function provided by the assignment package \textit{dist2} to find the distance among two sets of points. For each point in the base cloud we match it with the point in the target cloud that returns the lowest distance. 
	
	Our first approach was to use brute force and compare all points in one cloud to the points in the second cloud. However, this lead to problems regarding memory capacity. Thus, we chose to sample the points to be matched to work within such limitations.
	
	A second approach led us to sample the first \textit{k} elements in each cloud and compare them. This leas to incomplete clouds which were not representative of the whole.
	
	Finally, we decided to go with uniform random sampling to get $k$ samples from each cloud, and match them together. This by itself brings new problems which shall be discussed in Section \ref{reflection}
	
	\item \textbf{Center point clouds:} Secondly, we find the geometric centers of each cloud and subtract the centroid from each point. 
	
	\item \textbf{Matrix Decomposition:} Next, we apply Singular Value Decomposition using Matlab's built in function \textit{svd}. The matrix to be decomposed is $A$, dimensions $3 x 3$, which is built as the dot product of the centered clouds: 
	
	\begin{center}
		$A = centered\_pcd\_base^T * centered\_pcd\_match$
	\end{center}
	
	The resulting matrices are the standard $U, S, V$ matrices.	
	
	\item \textbf{Find transformation matrices:} We proceed to calculate the Rotation and Translation matrices. 
	
	Since at every iteration we perform an update on the transformation of the target cloud, all of these updates must be combined to create the final matrices. For the rotation matrix this means each intermediate matrix should be multiplied with the accumulated matrix. For the translation matrix this means each intermediate matrix must be added to the accumulated matrix.
	
	\item \textbf{Transform matrix and evaluate:} Finally we calculate the average distances between the base and the transformed clouds. 
	
	We implemented two smart stopping conditions: the first stops if the distance is already within a certain threshold. The selected threshold is $ threshold = 0.00005 $. The second condition limits the number of iterations to $50$, since literature shows that this is often where the rate of significant improvement slows down.
\end{enumerate}

We tested our implementation with the given \textit{source.mat} and \textit{target.mat} files. In Figure \ref{fig:test} we present a visualization of the results obtained with these point clouds while Figure \ref{fig:test_performance} shows the performance on the algorithm per iteration.

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\textwidth]{img/test_clouds.jpg}
	\caption{Alignment of source and target point clouds provided. Blue points correspond to source cloud and green points correspond to target cloud}
	\label{fig:test}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\textwidth]{img/test_performance.jpg}
	\caption{Average distance between matched points per iteration for source and target test datasets.}
	\label{fig:test_performance}
\end{figure}


\section{Merging scenes}
We use our function on the dataset provided. The set consists of 100 frames, where each point cloud corresponds to the image of a person from different angles.

Before calling our function, we pre-process the frames and remove outliers points. Since the cloud contains noise regarding the background we ignore all points which third coordinate is larger than a threshold. After tuning we decided to set $ distance\_threshold = 1.5 $

Next, we created a function that merges two given frames. An example for merging Frame 1 and Frame 2 is in Figure \ref{fig:simple_consecutive}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\textwidth]{img/consecutive_matched.jpg}
	\caption{Average distance between matched points per iteration for source and target test datasets.}
	\label{fig:simple_consecutive}
\end{figure}

\subsection{Estimating camera poses using consecutive frames}



\subsection{Estimating camera poses using iterative merging}



\section{Discussion}
\label{reflection}
Given the previous results we can reflect on our implementation and on the ICP algorithm in general.

\subsection{Reflection}
We noticed that the base cloud does not change over the iterations of the ICP, and so we thought that calculating and shifting the cloud once, in the first iteration of the algorithm, would suffice. On the other hand, since the target cloud does change, we calculate its centroid on every iteration and subtract it from every point. However, this approach led to errors in the transformation. After some reflection we realized our assumption was wrong. Since we performed the matching before the centering on every iteration, we were matching the centered base cloud with the uncentered target cloud.

Additionally we noticed that normalizing the rotation matrix did not have an effect on the end result or performance of the ICP function. Intuitively, we believed that normalizing would remove noise from the algorithm, however, this is not the case. 

\subsection{Drawbacks of ICP}
Sampling size is important. Given the memory complexity of the brute force comparison, we have to work within a certain limit of comparisons.

The performance heavily depends on the initial orientation of the point clouds. 
 
 
\subsection{Improvements}
With regards to our implementation, we recognize that the sampling process could be improved. Random sampling does not ensure that the best most significant points will be taken into account while matching. A smarter approach is to select points according to their normals or other feature information, as suggested in the literature. However, given the time limitations we were not able to implement such mechanisms.

In a more general scope, the ICP algorithm is very memory demanding, as it has been mentioned before. It makes sense that several improvements had been suggested to make smart selection of points and weighting of the matches. In it simplest, we can say that ICP is a very naive approach, guided by uninformed brute force. If treated as a search algorithm, ideas such as genetic algorithms or tabu search could find its parallel implementations in ICP.

\end{document}

